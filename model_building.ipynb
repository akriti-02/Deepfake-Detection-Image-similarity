{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "ViHLCL91WcDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the CSV file add a column names \"Target\"\n",
        "# we have 40-50 image of each person in the csv dataset in the image id column\n",
        "# along the image of the two people who were used for deepfake : person1 - original Video , person 2 - image used for deepfake , give then target score 1 and all the rest of the image\n",
        "# target score 0\n",
        "# Hence we will create models for binary classification\n",
        "\n",
        "# THis step should be done before loading the final csv file.\n",
        "\n",
        "\n",
        "# U will get a final csv file with following columns\n",
        "#  Image Name \tSSIM Score \tMSE Error \tMS-SSIM Score \tEuclidean Distance \tEuclidean Distance.1 \tTarget\n"
      ],
      "metadata": {
        "id": "lEs62aVjW_Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "bMKdwzklVFID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## reading a dataset\n",
        "data1 = pd.read_csv(\"load the csv dataset with extracted similarity scores\")"
      ],
      "metadata": {
        "id": "eybdyu8vWbhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8## Getting first five values of dataset\n",
        "\n",
        "data1.head()"
      ],
      "metadata": {
        "id": "c5te4Yi0W9mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.shape"
      ],
      "metadata": {
        "id": "9AxFMVEJXymf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking for missing values\n",
        "\n",
        "data1.isnull().sum()"
      ],
      "metadata": {
        "id": "eYzdzOE1X8lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking for duplicated data\n",
        "\n",
        "data1[data1.duplicated()]"
      ],
      "metadata": {
        "id": "RtukmymHXyjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.drop(columns='MS-SSIM Score',inplace=True)"
      ],
      "metadata": {
        "id": "GfUXMzALXygx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.drop(columns='Image Name',inplace=True)"
      ],
      "metadata": {
        "id": "RVY837DoX4hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting basic information about dataset\n",
        "\n",
        "data1.info()"
      ],
      "metadata": {
        "id": "PL_CUMmJYMr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cheking whether data is balanaced or not\n",
        "\n",
        "data1['Target'].value_counts()"
      ],
      "metadata": {
        "id": "zkurBNKOYMoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset is imbalanced\n",
        "## breaking data into independent and dependent variable\n",
        "X=data1.drop(columns=[\"Target\"])\n",
        "y=data1[\"Target\"]"
      ],
      "metadata": {
        "id": "cTfWbYc4YPzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "rEx-aDKMYPwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## since  the data is highly imbalance we wil use SMOTE analysis for balancing the dataset\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Perform SMOTE to balance the dataset\n",
        "smote = SMOTE()\n",
        "X,y = smote.fit_resample(X,y)\n"
      ],
      "metadata": {
        "id": "jTi1V-r8YPta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "id": "viF_X1LHYaPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.Series(y).value_counts())"
      ],
      "metadata": {
        "id": "DWQTwhjyYcZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8sYNTszYcXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mUzSkYacYcUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWh8oewIYcRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = data1.corr()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Draw the heatmap using seaborn\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "077VA__LWwPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\tSSIM Score \tMSE Error Euclidean Distance \tEuclidean Distance.1 \tTarget\n",
        "# Scatter Plot for SSIM Score and MSE Error\n",
        "plt.scatter(data1['SSIM Score'], data1['MSE Error'])\n",
        "plt.title('Scatter Plot: SSIM Score vs MSE Error')\n",
        "plt.xlabel('SSIM Score')\n",
        "plt.ylabel('MSE Error')\n",
        "plt.show()\n",
        "# Histogram for Euclidean Distance\n",
        "plt.hist(data1['Euclidean Distance'], bins=10, color='blue', edgecolor='black')\n",
        "plt.title('Histogram: Euclidean Distance')\n",
        "plt.xlabel('Euclidean Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Scatter Plot for Euclidean Distance and Conv Euclidean Distance\n",
        "plt.scatter(data1['Euclidean Distance'], data1['Euclidean Distance.1'])\n",
        "plt.title('Scatter Plot: Euclidean Distance vs Euclidean Distance.1')\n",
        "plt.xlabel('Euclidean Distance')\n",
        "plt.ylabel('Euclidean Distance.1')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Histogram for Euclidean Distance\n",
        "plt.hist(data1['SSIM Score'], bins=10, color='blue', edgecolor='black')\n",
        "plt.title('Histogram: SSIM Score')\n",
        "plt.xlabel('SSIM Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Histogram for Euclidean Distance\n",
        "plt.hist(data1['MSE Error'], bins=10, color='blue', edgecolor='black')\n",
        "plt.title('Histogram: MSE Error')\n",
        "plt.xlabel('MSE Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Histogram for Euclidean Distance\n",
        "plt.hist(data1['Euclidean Distance.1'], bins=10, color='blue', edgecolor='black')\n",
        "plt.title('Histogram: Euclidean Distance.1')\n",
        "plt.xlabel('Euclidean Distance.1')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b0d30oplWwMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting dataset as train test data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "CWAt9EgZWwJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a RandomForestRegressor model (you can also use DecisionTreeRegressor for a single tree)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature names and their importance scores\n",
        "importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
        "importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importances_df['Feature'], importances_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IHNrZrTGWwG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Scaling the data with Min_Max_Scaler as the columns as the features does not follow normal distribution and values of each column are in certain range itself.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "Min_Max_Scaler = MinMaxScaler()\n",
        "X_train_run_Min_Max_Scaler = Min_Max_Scaler.fit_transform(X_train)\n",
        "X_test_run_Min_Max_Scaler = Min_Max_Scaler.transform(X_test)\n",
        "\n",
        "## Scaling the data with standard scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "Standard_Scaler = StandardScaler()\n",
        "X_train_run_Standard_Scaler = Standard_Scaler.fit_transform(X_train)\n",
        "X_test_run_Standard_Scaler = Standard_Scaler.transform(X_test)\n",
        "\n",
        "## Scaling the data with Robust scaler as our data contains lots of outliers and Robust scaler is robust to ouliers\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "Robust_Scaler = RobustScaler()\n",
        "X_train_run_Robust_Scaler = Robust_Scaler.fit_transform(X_train)\n",
        "X_test_run_Robust_Scaler = Robust_Scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "miWuIyEeWwD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a common function to calculate the metrics after prediction\n",
        "from sklearn.metrics import accuracy_score\n",
        "def metrics(y_actual,y_predicted):\n",
        "    accuracy = accuracy_score(y_actual,y_predicted)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Q67Uj3m9WwBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = {\"X_without_scaling\":[X_train,X_test],\n",
        "          \"X_Min_Max_Scaler\":[X_train_run_Min_Max_Scaler,X_test_run_Min_Max_Scaler],\n",
        "          \"X_Standard_Scaler\":[X_train_run_Standard_Scaler,X_test_run_Standard_Scaler],\n",
        "          \"X_Robust_Scaler\":[X_train_run_Robust_Scaler,X_test_run_Robust_Scaler],\n",
        "          }"
      ],
      "metadata": {
        "id": "CJephMcUYmGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "def Decision_Tree_Classifier():\n",
        "    Decision_Tree_Classifier = DecisionTreeClassifier()\n",
        "    for key,value in X_data.items():\n",
        "        print('Decision Tree Model')\n",
        "        print(f'Working with: {key}')\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "        Decision_Tree_Classifier.fit(value[0],y_train)\n",
        "        print(f'Done with fitting the data using: {key}')\n",
        "        print(f'Prediction started with: {key}')\n",
        "        y_predicted = Decision_Tree_Classifier.predict(value[1])\n",
        "        print(f'Prediction completed with: {key}')\n",
        "        print(f\"Calculating metrics for {key}\")\n",
        "        accuracy1 = metrics(y_test,y_predicted)\n",
        "        print(f'accuracy: {accuracy1}')\n",
        "\n",
        "        # Create a confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_predicted)\n",
        "\n",
        "        # Plot the confusion matrix\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "        disp.plot(cmap='Reds_r', values_format='d')\n",
        "        plt.title('Confusion Matrix - Decision Tree')\n",
        "        plt.show()\n",
        "        print(\"Precision:\", precision_score(y_test, y_predicted))\n",
        "        print(\"Recall:\", recall_score(y_test, y_predicted))\n",
        "        print(\"F1-score:\", f1_score(y_test, y_predicted))\n",
        "        print(\"AUC-ROC:\", roc_auc_score(y_test, Decision_Tree_Classifier.predict_proba(X_test)[:, 1]))\n",
        "\n",
        "        print(\"==================================================================================================================================================\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YuO0jL6eYmC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decision_Tree_Classifier()"
      ],
      "metadata": {
        "id": "RTEJfdhIYmAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def Random_Forest_Classifier():\n",
        "    Random_Forest_Classifier = RandomForestClassifier()\n",
        "    for key,value in X_data.items():\n",
        "        print('Random Forest Model')\n",
        "        print(f'Working with: {key}')\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "        Random_Forest_Classifier.fit(value[0],y_train)\n",
        "        print(f'Done with fitting the data using: {key}')\n",
        "        print(f'Prediction started with: {key}')\n",
        "        y_predicted = Random_Forest_Classifier.predict(value[1])\n",
        "        print(f'Prediction completed with: {key}')\n",
        "        print(f\"Calculating metrics for {key}\")\n",
        "        accuracy2 = metrics(y_test,y_predicted)\n",
        "        print(f'accuracy: {accuracy2}')\n",
        "\n",
        "\n",
        "        # Create a confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_predicted)\n",
        "\n",
        "        # Plot the confusion matrix\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "        disp.plot(cmap='Blues', values_format='d')\n",
        "        plt.title('Confusion Matrix - Random Forest')\n",
        "        plt.show()\n",
        "        print(\"Precision:\", precision_score(y_test, y_predicted))\n",
        "        print(\"Recall:\", recall_score(y_test, y_predicted))\n",
        "        print(\"F1-score:\", f1_score(y_test, y_predicted))\n",
        "        print(\"AUC-ROC:\", roc_auc_score(y_test, Random_Forest_Classifier.predict_proba(X_test)[:, 1]))\n",
        "\n",
        "        print(\"==================================================================================================================================================\")\n"
      ],
      "metadata": {
        "id": "3-MxR19KYl-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Random_Forest_Classifier()"
      ],
      "metadata": {
        "id": "pqKpBH7kYvIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}